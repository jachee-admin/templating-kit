###### APEX

# Demo App: Seed data and provisioning


### File: `data/seed/target_table.csv`

```csv
col1,col2,col3,col4
ACME-001,2025-10-01,Alpha,10.5
ACME-002,2025-10-02,Beta,20
ACME-003,2025-10-03,Gamma,30.75
ACME-004,2025-10-04,Delta,0
```

---

### File: `tools/provision_dev.sql`

> Run with SQLcl:
> `sql user/password@tns_alias @tools/provision_dev.sql app_file=apex/apps/csv_import_demo.sql app_alias=csv_import_demo`

```sql
-- tools/provision_dev.sql
-- Usage:
--   sql user/pw@db @tools/provision_dev.sql app_file=apex/apps/csv_import_demo.sql app_alias=csv_import_demo
-- Variables:
--   &app_file : path to APEX app export (single-file .sql OR main loader of split export)
--   &app_alias: desired app alias (optional but recommended)

whenever sqlerror exit failure rollback
set define on verify off term on serveroutput on feed on pages 100 lines 200

prompt ===[ 0) Schema sanity ]===
-- (Optional) adjust default tablespace/quotas here if needed.

prompt ===[ 1) Core target table + error log ]===
begin
  execute immediate q'[
    create table target_table (
      col1 varchar2(100),
      col2 date,
      col3 varchar2(100),
      col4 number
    )
  ]';
exception when others then
  if sqlcode = -955 then null; else raise; end if;
end;
/

-- Error logging table (idempotent create)
declare
  e_exists exception; pragma exception_init(e_exists, -955);
begin
  dbms_errlog.create_error_log(dml_table_name => 'TARGET_TABLE');
exception when e_exists then null;
end;
/

prompt ===[ 2) Async CSV infrastructure (uploads/runs/errors) ]===
begin execute immediate q'[
  create table csv_uploads(
    upload_id    number generated by default as identity primary key,
    filename     varchar2(255),
    mime_type    varchar2(128),
    uploaded_by  varchar2(128),
    uploaded_at  timestamp default systimestamp,
    content      blob
  )
]'; exception when others then if sqlcode=-955 then null; else raise; end if; end;
/
begin execute immediate q'[
  create table csv_import_runs(
    run_id         number generated by default as identity primary key,
    upload_id      number not null references csv_uploads(upload_id),
    target_table   varchar2(128) not null,
    status         varchar2(20)  default 'QUEUED' check (status in ('QUEUED','RUNNING','SUCCEEDED','FAILED')),
    total_rows     number,
    processed_rows number default 0,
    error_rows     number default 0,
    message        varchar2(4000),
    created_at     timestamp default systimestamp,
    started_at     timestamp,
    finished_at    timestamp
  )
]'; exception when others then if sqlcode=-955 then null; else raise; end if; end;
/
begin execute immediate q'[
  create table csv_import_errors(
    run_id     number references csv_import_runs(run_id),
    row_num    number,
    row_text   varchar2(4000),
    err_msg    varchar2(4000),
    created_at timestamp default systimestamp
  )
]'; exception when others then if sqlcode=-955 then null; else raise; end if; end;
/

prompt ===[ 3) CSV import package (submit + worker) ]===
create or replace package pkg_csv_import as
  function submit_job(
    p_upload_id    in number,
    p_target_table in varchar2,
    p_commit_every in number default 1000,
    p_chunk_rows   in number default 5000
  ) return number;

  procedure worker(p_run_id in number);
end pkg_csv_import;
/
create or replace package body pkg_csv_import as
  function submit_job(
    p_upload_id    in number,
    p_target_table in varchar2,
    p_commit_every in number default 1000,
    p_chunk_rows   in number default 5000
  ) return number
  is
    l_run_id  number;
    l_job_name varchar2(128);
  begin
    insert into csv_import_runs (upload_id, target_table)
    values (p_upload_id, upper(p_target_table))
    returning run_id into l_run_id;

    l_job_name := 'CSV_IMPORT_' || l_run_id;

    dbms_scheduler.create_job(
      job_name           => l_job_name,
      job_type           => 'STORED_PROCEDURE',
      job_action         => 'PKG_CSV_IMPORT.WORKER',
      number_of_arguments=> 1,
      start_date         => systimestamp,
      enabled            => false,
      auto_drop          => true,
      comments           => 'Async CSV import run '||l_run_id
    );

    dbms_scheduler.set_job_argument_value(l_job_name, 1, to_char(l_run_id));
    dbms_scheduler.enable(l_job_name);
    commit;
    return l_run_id;
  end submit_job;

  procedure worker(p_run_id in number) is
    l_upload_id   csv_import_runs.upload_id%type;
    l_target      csv_import_runs.target_table%type;
    l_blob        blob;
    l_fname       varchar2(255);
    l_pos         number := 0;
    l_chunk       number := 5000;
    l_processed   number := 0;
    l_errors      number := 0;
    l_data        apex_data_parser.t_data;
    l_rows        number;
    type t_row is record (c1 varchar2(4000), c2 varchar2(4000), c3 varchar2(4000), c4 varchar2(4000));
    type t_rows is table of t_row index by pls_integer;
    v_rows t_rows;
  begin
    update csv_import_runs set status='RUNNING', started_at=systimestamp where run_id=p_run_id;

    select r.upload_id, r.target_table into l_upload_id, l_target
      from csv_import_runs r where r.run_id=p_run_id for update;

    select content, filename into l_blob, l_fname from csv_uploads where upload_id=l_upload_id;

    loop
      l_data := apex_data_parser.parse(
                  p_content         => l_blob,
                  p_file_name       => l_fname,
                  p_add_headers_row => 'Y',
                  p_skip_rows       => l_pos,
                  p_max_rows        => l_chunk );
      exit when l_data.count = 0;

      v_rows.delete; l_rows := l_data.count;
      for i in 1..l_rows loop
        v_rows(i).c1 := l_data(i).col001;
        v_rows(i).c2 := l_data(i).col002;
        v_rows(i).c3 := l_data(i).col003;
        v_rows(i).c4 := l_data(i).col004;
      end loop;

      begin
        forall j in 1..v_rows.count save exceptions
          insert into target_table(col1,col2,col3,col4)
          values (
            v_rows(j).c1,
            to_date(v_rows(j).c2,'YYYY-MM-DD'),
            v_rows(j).c3,
            to_number(v_rows(j).c4)
          );
        l_processed := l_processed + sql%rowcount;
      exception when others then
        declare
          n number := sql%bulk_exceptions.count;
        begin
          for k in 1..n loop
            insert into csv_import_errors(run_id,row_num,row_text,err_msg)
            values(
              p_run_id,
              l_pos + sql%bulk_exceptions(k).error_index,
              v_rows(sql%bulk_exceptions(k).error_index).c1||','||
              v_rows(sql%bulk_exceptions(k).error_index).c2||','||
              v_rows(sql%bulk_exceptions(k).error_index).c3||','||
              v_rows(sql%bulk_exceptions(k).error_index).c4,
              sqlerrm(-sql%bulk_exceptions(k).error_code)
            );
          end loop;
          l_errors := l_errors + n;
        end;
      end;

      update csv_import_runs
         set processed_rows=l_processed,
             error_rows    =l_errors,
             message       ='Parsed '||(l_pos + l_rows)||' rows from '||l_fname
       where run_id=p_run_id;

      commit;
      l_pos := l_pos + l_rows;
    end loop;

    update csv_import_runs
       set status='SUCCEEDED',
           finished_at=systimestamp,
           total_rows = nvl(total_rows,l_pos),
           message    ='Done: '||l_processed||' ok, '||l_errors||' errors'
     where run_id=p_run_id;
    commit;

  exception when others then
    update csv_import_runs
       set status='FAILED',
           finished_at=systimestamp,
           message=substr(sqlerrm,1,4000)
     where run_id=p_run_id;
    commit;
    raise;
  end worker;
end pkg_csv_import;
/
show errors

prompt ===[ 4) Seed data into TARGET_TABLE from CSV ]===
-- Load CSV via SQLcl CSV command if available; otherwise simple INSERTs:
-- Quick portable load using external table-on-the-fly is overkill; keep it simple:

-- Fallback: insert a couple rows mirroring the CSV (for smoke test)
begin
  insert into target_table(col1,col2,col3,col4) values ('SEED-OK-001', date '2025-10-10', 'Seed', 1);
  insert into target_table(col1,col2,col3,col4) values ('SEED-OK-002', date '2025-10-11', 'Seed', 2.5);
exception when dup_val_on_index then null; when others then null;
end;
/
commit;

prompt Hint: To test end-to-end, upload data/seed/target_table.csv through the APEX page (70/72 flow).

prompt ===[ 5) Import APEX application ]===
declare
  l_alias varchar2(255) := nvl('&&app_alias', null);
begin
  apex_application_install.set_auto_install_sup_obj(false);
  if l_alias is not null then
    apex_application_install.set_application_alias(l_alias);
  end if;
  -- You can also set workspace, app id/offset, parsing schema here if needed:
  -- apex_application_install.set_workspace_id(...);
  -- apex_application_install.set_schema('YOUR_SCHEMA');
end;
/

prompt Running: &app_file
@&app_file

prompt ===[ 6) Grant execute (optional) and scheduler sanity ]===
-- Ensure the schema has job privileges and parser access
declare
  v number;
begin
  select value into v from v$parameter where name='job_queue_processes';
  dbms_output.put_line('job_queue_processes='||v);
exception when others then null;
end;
/

prompt ===[ DONE ]===
```

---

### (Optional) File: `tools/provision_dev.sh`

```bash
#!/usr/bin/env bash
# Usage: ./tools/provision_dev.sh user/pw@db apex/apps/csv_import_demo.sql csv_import_demo
set -euo pipefail
CONN="${1:?conn}"
APP_FILE="${2:?app file}"
APP_ALIAS="${3:-}"
sql "$CONN" @tools/provision_dev.sql app_file="$APP_FILE" app_alias="$APP_ALIAS"
```

---

### How to use

1. Put the files exactly as shown:

```
repo/
├─ data/seed/target_table.csv
└─ tools/
   ├─ provision_dev.sql
   └─ provision_dev.sh   # optional
```

2. Export your APEX app (single-file or split main loader) to `apex/apps/csv_import_demo.sql`.

3. Run the provision:

```
sql user/pw@db @tools/provision_dev.sql app_file=apex/apps/csv_import_demo.sql app_alias=csv_import_demo
```

4. Open the app, go to the **Upload & Enqueue** page, choose `data/seed/target_table.csv`, and watch **Run Status** tick.
