###### APEX

# Async CSV Import (Large Files) via `DBMS_SCHEDULER` + `apex_data_parser`

Run big CSV imports in the background: user uploads a file, you persist it, enqueue a job, stream/parse in chunks, and surface progress & errors in APEX.

---

## TL;DR

* **Persist** uploads to your own BLOB table (don’t rely on `APEX_APPLICATION_TEMP_FILES` for async).
* **Enqueue** a `DBMS_SCHEDULER` job with a `RUN_ID` and `UPLOAD_ID`.
* Worker **streams** CSV with `apex_data_parser.parse` in chunks (`p_skip_rows`, `p_max_rows`) and bulk inserts.
* Track **status** (`QUEUED/RUNNING/SUCCEEDED/FAILED`) and rows processed; expose a live **status report** + **error log** region.
* Use `DBMS_ERRLOG` or a custom error table; never abort on the first bad row.

---

## 1) Tables: Uploads, Runs, and Errors

```sql
-- Durable file storage (survives APEX session end)
create table csv_uploads (
  upload_id    number generated by default as identity primary key,
  filename     varchar2(255),
  mime_type    varchar2(128),
  uploaded_by  varchar2(128),
  uploaded_at  timestamp default systimestamp,
  content      blob
);

-- Import runs (one per job)
create table csv_import_runs (
  run_id         number generated by default as identity primary key,
  upload_id      number not null references csv_uploads(upload_id),
  target_table   varchar2(128) not null,
  status         varchar2(20)  default 'QUEUED' check (status in ('QUEUED','RUNNING','SUCCEEDED','FAILED')),
  total_rows     number,
  processed_rows number default 0,
  error_rows     number default 0,
  message        varchar2(4000),
  created_at     timestamp default systimestamp,
  started_at     timestamp,
  finished_at    timestamp
);

-- Per-row error capture (optional if you prefer DBMS_ERRLOG)
create table csv_import_errors (
  run_id     number references csv_import_runs(run_id),
  row_num    number,
  row_text   varchar2(4000),
  err_msg    varchar2(4000),
  created_at timestamp default systimestamp
);

-- (Optional) Error logging table for the target table
-- exec dbms_errlog.create_error_log(dml_table_name => 'TARGET_TABLE');
```

---

## 2) Package API: Submit + Worker

```plsql
create or replace package pkg_csv_import as
  -- Enqueue a background job; returns RUN_ID
  function submit_job(
    p_upload_id    in number,
    p_target_table in varchar2,
    p_commit_every in number default 1000,
    p_chunk_rows   in number default 5000
  ) return number;

  -- Worker entry point used by DBMS_SCHEDULER
  procedure worker(p_run_id in number);
end pkg_csv_import;
/
```

```plsql
create or replace package body pkg_csv_import as

  function submit_job(
    p_upload_id    in number,
    p_target_table in varchar2,
    p_commit_every in number default 1000,
    p_chunk_rows   in number default 5000
  ) return number
  is
    l_run_id number;
    l_job_name varchar2(128);
  begin
    insert into csv_import_runs (upload_id, target_table)
    values (p_upload_id, upper(p_target_table))
    returning run_id into l_run_id;

    l_job_name := 'CSV_IMPORT_' || l_run_id;

    dbms_scheduler.create_job(
      job_name        => l_job_name,
      job_type        => 'STORED_PROCEDURE',
      job_action      => 'PKG_CSV_IMPORT.WORKER',
      number_of_arguments => 1,
      start_date      => systimestamp,
      enabled         => false,
      auto_drop       => true,
      comments        => 'Async CSV import run '||l_run_id
    );

    dbms_scheduler.set_job_argument_value(l_job_name, 1, to_char(l_run_id));
    dbms_scheduler.enable(l_job_name);

    commit;
    return l_run_id;
  end submit_job;

  procedure worker(p_run_id in number) is
    l_upload_id   csv_import_runs.upload_id%type;
    l_target      csv_import_runs.target_table%type;
    l_blob        blob;
    l_fname       varchar2(255);
    l_mime        varchar2(128);
    l_pos         number := 0;         -- rows already consumed
    l_chunk       number := 5000;      -- rows per parse
    l_total       number := null;      -- set once we detect it (optional)
    l_processed   number := 0;
    l_errors      number := 0;
    l_commit_n    number := 1000;
    l_data        apex_data_parser.t_data;
    l_rows        number;
    -- map columns: adjust for your CSV schema
    type t_row is record (c1 varchar2(4000), c2 varchar2(4000), c3 varchar2(4000), c4 varchar2(4000));
    type t_rows is table of t_row index by pls_integer;
    v_rows t_rows;
  begin
    update csv_import_runs
    set status='RUNNING', started_at = systimestamp
    where run_id = p_run_id;

    select r.upload_id, r.target_table
      into l_upload_id, l_target
    from csv_import_runs r
    where r.run_id = p_run_id
    for update;

    select content, filename, mime_type
      into l_blob, l_fname, l_mime
    from csv_uploads
    where upload_id = l_upload_id;

    -- Main loop: parse in chunks using skip_rows + max_rows
    loop
      l_data := apex_data_parser.parse(
                  p_content         => l_blob,
                  p_file_name       => l_fname,
                  p_add_headers_row => 'Y',
                  p_skip_rows       => l_pos,
                  p_max_rows        => l_chunk );

      exit when l_data.count = 0;

      -- Bulk collect into PL/SQL table; convert types later in SQL
      v_rows.delete;
      l_rows := l_data.count;

      for i in 1 .. l_rows loop
        v_rows(i).c1 := l_data(i).col001;
        v_rows(i).c2 := l_data(i).col002;
        v_rows(i).c3 := l_data(i).col003;
        v_rows(i).c4 := l_data(i).col004;
      end loop;

      -- Example bulk insert using INSERT-SELECT from TABLE() of rows
      -- Replace with your real target schema & conversions.
      begin
        forall j in 1 .. v_rows.count save exceptions
          insert into target_table (col1, col2, col3, col4)
          values (v_rows(j).c1,
                  to_date(v_rows(j).c2, 'YYYY-MM-DD'),
                  v_rows(j).c3,
                  to_number(v_rows(j).c4));

        l_processed := l_processed + sql%rowcount;

      exception
        when others then
          -- Iterate exceptions to record row-level failures
          declare
            bulk_errors number := sql%bulk_exceptions.count;
          begin
            for k in 1 .. bulk_errors loop
              insert into csv_import_errors(run_id, row_num, row_text, err_msg)
              values (
                p_run_id,
                l_pos + sql%bulk_exceptions(k).error_index,
                v_rows(sql%bulk_exceptions(k).error_index).c1||','||
                v_rows(sql%bulk_exceptions(k).error_index).c2||','||
                v_rows(sql%bulk_exceptions(k).error_index).c3||','||
                v_rows(sql%bulk_exceptions(k).error_index).c4,
                sqlerrm(-sql%bulk_exceptions(k).error_code)
              );
            end loop;
            l_errors := l_errors + bulk_errors;
          end;
      end;

      -- Heartbeat progress
      update csv_import_runs
         set processed_rows = l_processed,
             error_rows     = l_errors,
             message        = 'Parsed '|| (l_pos + l_rows) ||' rows from '||l_fname
       where run_id = p_run_id;

      commit;  -- commit periodically; tune if needed

      l_pos := l_pos + l_rows;

      -- Optional: stop after N rows if you compute total elsewhere
      -- exit when l_total is not null and l_pos >= l_total;
    end loop;

    update csv_import_runs
       set status='SUCCEEDED',
           finished_at = systimestamp,
           total_rows = nvl(total_rows, l_pos),
           message = 'Done: '||l_processed||' ok, '||l_errors||' errors'
     where run_id = p_run_id;

    commit;

  exception
    when others then
      update csv_import_runs
         set status='FAILED',
             finished_at = systimestamp,
             message = substr(sqlerrm,1,4000)
       where run_id = p_run_id;
      commit;
      raise;
  end worker;

end pkg_csv_import;
/
```

**Notes:**

* The worker runs **without an APEX session**—avoid APEX session APIs.
* Adjust the `insert into target_table` mapping and conversions to match your schema.
* For truly massive files, consider **external tables** or **SQL*Loader**; this pattern prioritizes APEX-friendly ergonomics.

---

## 3) APEX: Upload, Enqueue, and UI

### Upload (Process)

1. Page file item `P71_FILE` (Storage: `APEX_APPLICATION_TEMP_FILES`).
2. Button **Save Upload** → PL/SQL process to persist in `CSV_UPLOADS` and enqueue job:

```plsql
declare
  l_temp apex_application_temp_files%rowtype;
  l_upload_id number;
  l_run_id number;
begin
  select * into l_temp
    from apex_application_temp_files
   where name = :P71_FILE;

  insert into csv_uploads(filename, mime_type, uploaded_by, content)
  values (l_temp.filename, l_temp.mime_type, :APP_USER, l_temp.blob_content)
  returning upload_id into l_upload_id;

  l_run_id := pkg_csv_import.submit_job(
                p_upload_id    => l_upload_id,
                p_target_table => 'TARGET_TABLE',
                p_commit_every => 1000,
                p_chunk_rows   => 5000);

  :P71_RUN_ID := l_run_id;

  -- Optional: delete temp file
  delete from apex_application_temp_files where name = :P71_FILE;
end;
```

### Status Region (Report SQL, refresh every 3–5s)

```sql
select run_id, status, processed_rows, error_rows, total_rows,
       round(100 * processed_rows / nullif(total_rows,0),1) as pct,
       message,
       created_at, started_at, finished_at
from csv_import_runs
where run_id = :P71_RUN_ID
```

### Errors Region (Report SQL)

```sql
select row_num, err_msg, row_text, created_at
from csv_import_errors
where run_id = :P71_RUN_ID
order by row_num
```

---

## 4) Scheduler & Security Considerations

* Ensure **job queue** is enabled: `alter system set job_queue_processes=20;`
* The schema running jobs needs privileges on target tables and `apex_data_parser`.
* Lock down upload size and MIME type; log `uploaded_by` and IP (if required).
* Consider a **retention policy**: purge `csv_uploads.content` BLOB after success.

---

## 5) Operational Tips

* **Chunk size** (`p_chunk_rows`) ~2–20k rows typically balances CPU/memory.
* Use **`save exceptions`** or `DBMS_ERRLOG` to avoid aborting the whole job.
* For repeat feeds, add a **dedupe key** or `MERGE` instead of `INSERT`.
* Expose a **“Cancel Job”** button (call `dbms_scheduler.stop_job(job_name=>...)`) if you also store job names.

---

## Notes

* `apex_data_parser` works outside APEX sessions (it’s a DB package).
* This pattern keeps the UI responsive and makes large imports predictable.
* For XLSX, this is identical—`apex_data_parser` auto-profiles by filename.

---

```yaml
---
id: templates/apex/71-apex-csv-async-scheduler.md
lang: plsql
platform: apex
scope: data-load-async
since: "v0.1"
tested_on: "APEX 24.2 / Oracle 23ai"
tags: [apex, csv, async, scheduler, apex_data_parser, background-job, error-logging]
description: "Asynchronous CSV imports with durable uploads, DBMS_SCHEDULER worker, chunked parsing, and live progress/error reporting in APEX."
---
```
